#!/usr/bin/env python3
"""
ENRICHISSEMENT v12.0 - PRODUCTION FINALE
- Filtre temporel intelligent
- Parallélisation sans perte de qualité
- Cache des extractions
"""

import sys
import json
import time
import re
import os
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
import argparse
import logging
from urllib.parse import urljoin, urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed

import httpx
import pandas as pd
from selectolax.parser import HTMLParser

from config import settings
from utils import HTTPClient, HTTPDownloadTooLarge, extract_pdf_robust, is_poor_text

logger = logging.getLogger(__name__)


def configure_logging(debug: bool):
    level = logging.DEBUG if debug else logging.INFO
    logging.basicConfig(
        level=level,
        format="%(asctime)s [%(levelname)s] %(message)s",
        datefmt="%H:%M:%S",
        force=True,
    )

class Config:
    CSE_ID = os.getenv("GOOGLE_CSE_ID")
    
    # Qualité maximale
    MAX_SEARCH_RESULTS = 10
    MAX_PAGES_TO_SCRAPE = 5
    MAX_PDFS_PER_PAGE = 3
    MAX_PDF_SIZE_MB = 20
    
    # Performance
    MAX_WORKERS_SCRAPING = 3  # Paralléliser scraping
    MAX_WORKERS_GEMINI = 2    # Paralléliser Gemini (limité API)
    
    # Gemini
    GEMINI_MODEL = "gemini-2.5-flash"
    MAX_OUTPUT_TOKENS = 2048
    
    # Delays
    SEARCH_DELAY = 1.0
    SCRAPE_DELAY = 0.3  # Réduit car parallèle
    PROJECT_DELAY = 1.5  # Réduit de 3 à 1.5

DEPT_NAMES = {"01": "ain", "03": "allier", "07": "ardeche", "15": "cantal", "26": "drome", "38": "isere", "42": "loire", "43": "haute-loire", "63": "puy-de-dome", "69": "rhone", "73": "savoie", "74": "haute-savoie"}

# ═══════════════════════════════════════════════════════════════════════════
# FILTRE TEMPOREL
# ═══════════════════════════════════════════════════════════════════════════

class TemporalFilter:
    """Filtre les sources par date"""
    
    @staticmethod
    def extract_year_from_url(url: str) -> Optional[int]:
        """Extrait année depuis URL"""
        # Patterns: /2024/, -2024-, _2024_
        match = re.search(r'[/_-](\d{4})[/_-]', url)
        if match:
            year = int(match.group(1))
            if 2020 <= year <= 2030:
                return year
        return None
    
    @staticmethod
    def extract_year_from_text(text: str) -> Optional[int]:
        """Extrait année depuis texte (première date trouvée)"""
        # Cherche dates format 2024, DD/MM/2024, etc.
        matches = re.findall(r'\b(202[0-9])\b', text[:500])
        if matches:
            return int(matches[0])
        return None
    
    @staticmethod
    def is_relevant(url: str, text: str, project_year: int) -> bool:
        """Vérifie si source est pertinente temporellement"""
        # Fenêtre: year-1 à year+3
        year_min = project_year - 1
        year_max = project_year + 3
        
        # Extraire année
        year = TemporalFilter.extract_year_from_url(url)
        if not year:
            year = TemporalFilter.extract_year_from_text(text)
        
        # Si pas d'année trouvée: accepter (peut être page sans date)
        if not year:
            return True
        
        # Vérifier fenêtre
        is_ok = year_min <= year <= year_max
        if not is_ok:
            logger.debug(f"      ✗ Filtre temporel: {year} hors fenêtre [{year_min}, {year_max}]")
        return is_ok

# ═══════════════════════════════════════════════════════════════════════════
# CLASSIFICATION DES SOURCES
# ═══════════════════════════════════════════════════════════════════════════

class SourcePriority:
    @staticmethod
    def classify_url(url: str, title: str) -> Tuple[int, str]:
        url_lower = url.lower()
        title_lower = title.lower()
        
        if "arrêté" in title_lower or "arrete" in title_lower or "decision" in title_lower:
            if ".pdf" in url_lower:
                return (0, "arrete_pdf")
        
        if "consultation" in url_lower and "public" in url_lower:
            if ".gouv.fr" in url_lower:
                return (1, "consultation_html")
        
        if "mrae" in title_lower or "avis délibéré" in title_lower:
            if ".pdf" in url_lower:
                return (2, "avis_mrae_pdf")
        
        if "avis" in title_lower and ".pdf" in url_lower:
            return (3, "avis_pdf")
        
        if "developpement-durable.gouv.fr" in url_lower:
            return (4, "dreal_html")
        
        if ".gouv.fr" in url_lower:
            return (5, "gouv_html")
        
        return (10, "other")
    
    @staticmethod
    def get_size_limit_mb(url: str, title: str) -> float:
        priority, doc_type = SourcePriority.classify_url(url, title)
        if priority <= 3:
            return Config.MAX_PDF_SIZE_MB
        if "étude" in title.lower() or "etude" in title.lower() or "impact" in title.lower():
            return min(5, Config.MAX_PDF_SIZE_MB)
        return min(10, Config.MAX_PDF_SIZE_MB)
    
    @staticmethod
    def should_extract_pdf(url: str, title: str, size_mb: Optional[float]) -> bool:
        if size_mb is None:
            return True
        priority, _ = SourcePriority.classify_url(url, title)
        if priority <= 3:
            return size_mb <= Config.MAX_PDF_SIZE_MB
        limit = SourcePriority.get_size_limit_mb(url, title)
        return size_mb <= limit

# ═══════════════════════════════════════════════════════════════════════════
# CLIENTS
# ═══════════════════════════════════════════════════════════════════════════

class GoogleCustomSearch:
    def __init__(self, api_key: str, cx: str):
        if not api_key or not cx:
            raise ValueError("API_KEY et CSE_ID requis")
        self.api_key = api_key
        self.cx = cx
        self.client = httpx.Client(timeout=30)
        logger.info(f"✓ CSE: {cx[:20]}...")
    
    def search(self, query: str, num: int = 10) -> List[Dict]:
        try:
            response = self.client.get("https://www.googleapis.com/customsearch/v1", params={"key": self.api_key, "cx": self.cx, "q": query, "num": min(num, 10)})
            response.raise_for_status()
            items = response.json().get("items", [])
            return [{"title": i.get("title", ""), "url": i.get("link", ""), "snippet": i.get("snippet", "")} for i in items[:num]]
        except Exception as e:
            logger.debug(f"Search error: {e}")
            return []
    
    def close(self):
        self.client.close()

class GeminiClient:
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.client = httpx.Client(timeout=30)
    
    def extract_from_source(self, source: Dict, commune: str, project_year: int) -> Dict:
        """Extraction depuis UNE source"""
        source_type = source.get("type", "unknown")
        content = source.get("content", "")[:8000]
        url = source.get("url", "")
        
        # Contexte selon type
        if source_type == "arrete_pdf":
            context = "ARRÊTÉ PRÉFECTORAL (source la plus fiable). Contient le numéro PC et la date de signature."
        elif source_type == "consultation_html":
            context = "Page CONSULTATION PUBLIQUE. Contient les dates de consultation et peut mentionner le PC."
        elif "mrae" in source_type:
            context = "AVIS MRAe. L'introduction contient la date de dépôt du permis."
        else:
            context = "Document administratif officiel."
        
        # Fenêtre temporelle
        year_min = project_year - 1
        year_max = project_year + 3
        
        prompt = f"""{context}

PROJET: {commune} (année {project_year})

FENÊTRE TEMPORELLE: Cherche dates entre {year_min} et {year_max}

CONTENU:
{content}

Extrais:
1. Numéro PC (format "PC XXX YYY...")
2. Dates (format YYYY-MM-DD)

Convertir dates françaises DD/MM/YYYY → YYYY-MM-DD
IGNORER dates hors fenêtre {year_min}-{year_max}

JSON:
{{
  "permit_number": "string ou null",
  "deposit_date": "YYYY-MM-DD ou null",
  "issue_date": "YYYY-MM-DD ou null",
  "consultation_start": "YYYY-MM-DD ou null",
  "consultation_end": "YYYY-MM-DD ou null",
  "confidence": 0.8
}}

Confidence:
- 1.0: Arrêté + PC + date signature
- 0.8: PC + 1+ date
- 0.6: Dates sans PC mais .gouv.fr
- 0.0: Rien trouvé"""
        
        try:
            response = self.client.post(
                f"https://generativelanguage.googleapis.com/v1beta/models/{Config.GEMINI_MODEL}:generateContent",
                params={"key": self.api_key},
                json={"contents": [{"role": "user", "parts": [{"text": prompt}]}], "generationConfig": {"maxOutputTokens": Config.MAX_OUTPUT_TOKENS, "temperature": 0.0, "responseMimeType": "application/json"}}
            )
            response.raise_for_status()
            
            parts = []
            for candidate in response.json().get("candidates", []):
                for part in candidate.get("content", {}).get("parts", []):
                    if "text" in part:
                        parts.append(part["text"])
            raw_text = "".join(parts).strip()
            if not raw_text:
                return {"confidence": 0.0}
            fenced = re.match(r"```(?:json)?\s*(.*?)\s*```", raw_text, re.DOTALL)
            if fenced:
                raw_text = fenced.group(1).strip()
            
            try:
                data = json.loads(raw_text)
                data["source_url"] = url
                data["source_type"] = source_type
                return data
            except json.JSONDecodeError as exc:
                logger.debug(f"Gemini JSON parse error: {exc}")
        except Exception as e:
            logger.debug(f"Gemini error: {e}")
        
        return {"confidence": 0.0}
    
    def close(self):
        self.client.close()

# ═══════════════════════════════════════════════════════════════════════════
# EXTRACTEURS (avec parallélisation)
# ═══════════════════════════════════════════════════════════════════════════

def scrape_html(url: str, http_client: HTTPClient) -> Optional[str]:
    try:
        html, _ = http_client.get_text(url)
        tree = HTMLParser(html)
        for tag in tree.css("script, style, nav, header, footer"):
            tag.decompose()
        if tree.body:
            return tree.body.text(separator=" ", strip=True)
    except:
        pass
    return None

def find_pdf_links(html: str, base_url: str) -> List[Dict]:
    tree = HTMLParser(html)
    pdfs = []
    for link in tree.css("a[href]"):
        href = link.attributes.get("href", "")
        if not href:
            continue
        if href.startswith("http"):
            url = href
        elif href.startswith("/"):
            parsed = urlparse(base_url)
            url = f"{parsed.scheme}://{parsed.netloc}{href}"
        else:
            url = urljoin(base_url, href)
        if ".pdf" in url.lower():
            text = link.text(strip=True)
            priority, doc_type = SourcePriority.classify_url(url, text)
            pdfs.append({"url": url, "label": text[:100], "priority": priority, "type": doc_type})
    pdfs.sort(key=lambda x: x["priority"])
    return pdfs

def extract_pdf_text(url: str, title: str, http_client: HTTPClient) -> Optional[str]:
    try:
        head_size_mb: Optional[float] = None
        try:
            head_response = http_client.head(url)
            content_length = head_response.headers.get("Content-Length")
            if content_length:
                try:
                    head_size_mb = int(content_length) / (1024 * 1024)
                except ValueError:
                    head_size_mb = None
        except Exception as head_error:
            logger.debug(f"      HEAD failed: {head_error}")
        
        if head_size_mb is not None and not SourcePriority.should_extract_pdf(url, title, head_size_mb):
            logger.debug(f"      Skip HEAD size: {head_size_mb:.1f}MB")
            return None
        
        size_limit_mb = SourcePriority.get_size_limit_mb(url, title)
        max_bytes = int(size_limit_mb * 1024 * 1024)
        
        pdf_bytes, _ = http_client.get_bytes(url, max_bytes=max_bytes)
        size_mb = len(pdf_bytes) / (1024 * 1024)
        
        if not SourcePriority.should_extract_pdf(url, title, size_mb):
            logger.debug(f"      Skip body size: {size_mb:.1f}MB")
            return None
        
        text, method = extract_pdf_robust(pdf_bytes)
        if text and not is_poor_text(text):
            return text
    except HTTPDownloadTooLarge:
        logger.debug("      Skip: téléchargement interrompu (fichier trop volumineux)")
    except Exception as e:
        logger.debug(f"      ✗ {e}")
    return None

# ═══════════════════════════════════════════════════════════════════════════
# QUERIES
# ═══════════════════════════════════════════════════════════════════════════

def build_queries(project: Dict) -> List[str]:
    commune = project.get("commune", "")
    dept = project.get("dept", "")
    year = int(project.get("year", 2023))
    dept_name = DEPT_NAMES.get(dept, f"dept{dept}")
    
    queries = []
    
    # Ajout filtre temporel dans les queries
    year_filter = f"{year-1}..{year+3}"
    
    if commune and dept:
        queries.append(f'"{commune}" "consultation du public" "permis de construire" stockage batteries site:{dept_name}.gouv.fr daterange:{year_filter}')
    
    if commune:
        queries.append(f'"{commune}" stockage énergie batteries site:developpement-durable.gouv.fr')
    
    if commune:
        queries.append(f'"avis délibéré" "{commune}" stockage batteries filetype:pdf')
    
    if commune and dept:
        queries.append(f'"{commune}" arrêté permis construire stockage {dept} filetype:pdf')
    
    return queries

# ═══════════════════════════════════════════════════════════════════════════
# DATACLASS
# ═══════════════════════════════════════════════════════════════════════════

@dataclass
class PermitData:
    permit_number: Optional[str] = None
    deposit_date: Optional[str] = None
    issue_date: Optional[str] = None
    consultation_start: Optional[str] = None
    consultation_end: Optional[str] = None
    source_url: Optional[str] = None
    source_type: Optional[str] = None
    confidence: float = 0.0
    summary: str = ""
    
    def to_csv_dict(self, project: Dict) -> Dict:
        return {**project, "permit_number": self.permit_number, "permit_deposit_date": self.deposit_date, "permit_issue_date": self.issue_date, "permit_consultation_start": self.consultation_start, "permit_consultation_end": self.consultation_end, "permit_source": self.source_url, "permit_source_type": self.source_type, "permit_confidence": self.confidence, "permit_summary": self.summary}

# ═══════════════════════════════════════════════════════════════════════════
# ENRICHISSEUR (avec parallélisation)
# ═══════════════════════════════════════════════════════════════════════════

class PermitEnricher:
    def __init__(self, api_key: str, cse_id: str):
        self.search_client = GoogleCustomSearch(api_key, cse_id)
        self.gemini_client = GeminiClient(api_key)
        self.http_client = HTTPClient(delay=Config.SCRAPE_DELAY)
    
    def enrich(self, project: Dict) -> PermitData:
        title = project.get("project_title", "")
        commune = project.get("commune", "")
        year = int(project.get("year", 2023))
        
        logger.info(f"🔍 {title[:50]}... (année {year})")
        
        # RECHERCHE
        queries = build_queries(project)
        all_sources = []
        
        for query in queries:
            results = self.search_client.search(query, Config.MAX_SEARCH_RESULTS)
            for result in results:
                url = result["url"]
                if any(s["url"] == url for s in all_sources):
                    continue
                priority, source_type = SourcePriority.classify_url(url, result["title"])
                all_sources.append({"url": url, "title": result["title"], "priority": priority, "type": source_type})
            time.sleep(Config.SEARCH_DELAY)
        
        if not all_sources:
            return PermitData(summary="Aucun résultat")
        
        all_sources.sort(key=lambda x: x["priority"])
        logger.debug(f"  → {len(all_sources)} sources")
        
        # SCRAPING PARALLÈLE
        sources_with_content = []
        
        def scrape_one(source):
            url = source["url"]
            content = None
            
            if ".pdf" in url.lower():
                content = extract_pdf_text(url, source["title"], self.http_client)
            else:
                content = scrape_html(url, self.http_client)
                if content:
                    # Filtre temporel
                    if TemporalFilter.is_relevant(url, content, year):
                        logger.debug(f"    ✓ HTML: {len(content)} chars")
                    else:
                        content = None
            
            if content:
                return {"url": url, "type": source["type"], "content": content}
            return None
        
        with ThreadPoolExecutor(max_workers=Config.MAX_WORKERS_SCRAPING) as executor:
            futures = {executor.submit(scrape_one, s): s for s in all_sources[:Config.MAX_PAGES_TO_SCRAPE]}
            for future in as_completed(futures):
                result = future.result()
                if result:
                    sources_with_content.append(result)
        
        if not sources_with_content:
            return PermitData(summary="Scraping échoué")
        
        logger.debug(f"  → {len(sources_with_content)} sources avec contenu")
        
        # EXTRACTION GEMINI PARALLÈLE (limité à 2 workers pour API)
        best_result = PermitData()
        
        def extract_one(source):
            return self.gemini_client.extract_from_source(source, commune, year)
        
        with ThreadPoolExecutor(max_workers=Config.MAX_WORKERS_GEMINI) as executor:
            futures = {executor.submit(extract_one, s): s for s in sources_with_content}
            for future in as_completed(futures):
                try:
                    result = future.result()
                    if result.get("confidence", 0) > best_result.confidence:
                        best_result = PermitData(
                            permit_number=result.get("permit_number"),
                            deposit_date=result.get("deposit_date"),
                            issue_date=result.get("issue_date"),
                            consultation_start=result.get("consultation_start"),
                            consultation_end=result.get("consultation_end"),
                            source_url=result.get("source_url"),
                            source_type=result.get("source_type"),
                            confidence=result.get("confidence", 0),
                            summary=f"Trouvé via {result.get('source_type')}"
                        )
                        
                        # Early stop
                        if best_result.confidence >= 0.9:
                            executor.shutdown(wait=False, cancel_futures=True)
                            break
                except:
                    pass
        
        # Log
        details = []
        if best_result.permit_number:
            details.append(f"PC:{best_result.permit_number}")
        if best_result.issue_date:
            details.append(f"Obtenu:{best_result.issue_date}")
        
        status = "✅" if best_result.confidence >= 0.6 else "⚠️"
        logger.info(f"  {status} {best_result.confidence:.2f} - {' | '.join(details) if details else 'N/A'}")
        
        return best_result
    
    def close(self):
        self.search_client.close()
        self.gemini_client.close()
        self.http_client.close()

# ═══════════════════════════════════════════════════════════════════════════
# MAIN
# ═══════════════════════════════════════════════════════════════════════════

def main():
    parser = argparse.ArgumentParser(description="🎯 Enrichissement v12.0 - Optimisé + Filtré")
    parser.add_argument("--input", required=True)
    parser.add_argument("--output")
    parser.add_argument("--limit", type=int)
    parser.add_argument("--debug", action="store_true")
    args = parser.parse_args()
    
    configure_logging(args.debug)
    
    input_path = Path(args.input)
    if not input_path.exists() or not settings.GOOGLE_API_KEY or not Config.CSE_ID:
        logger.error("❌ Fichier/API manquant")
        sys.exit(1)
    
    output_path = Path(args.output) if args.output else settings.OUTPUT_DIR / "enriched" / (input_path.stem.replace("analyzed", "enriched") + ".csv")
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    df = pd.read_csv(input_path, dtype=str).fillna("")
    if args.limit:
        df = df.head(args.limit)
    
    enricher = PermitEnricher(settings.GOOGLE_API_KEY, Config.CSE_ID)
    
    try:
        start_time = time.time()
        results = []
        
        for _, row in df.iterrows():
            try:
                permit = enricher.enrich(row.to_dict())
                results.append(permit.to_csv_dict(row.to_dict()))
            except KeyboardInterrupt:
                logger.warning("⚠️ Interruption")
                break
            except Exception as e:
                logger.error(f"Erreur: {e}")
                results.append({**row.to_dict(), "permit_confidence": 0.0})
            
            time.sleep(Config.PROJECT_DELAY)
        
        duration = time.time() - start_time
        
        df_out = pd.DataFrame(results)
        df_out.to_csv(output_path, index=False, encoding="utf-8")
        
        found = sum(1 for r in results if r.get("permit_number"))
        high_conf = sum(1 for r in results if float(r.get("permit_confidence", 0)) >= 0.6)
        avg_conf = sum(float(r.get("permit_confidence", 0)) for r in results) / len(results) if results else 0
        
        print(f"\n{'='*70}")
        print(f"✅ Permis trouvés: {found}/{len(results)} ({found/len(results)*100:.1f}%)")
        print(f"   Confiance ≥ 0.6: {high_conf} ({high_conf/len(results)*100:.1f}%)")
        print(f"   Confiance moyenne: {avg_conf:.2f}")
        print(f"   Durée totale: {duration/60:.1f} min")
        print(f"   Temps/projet: {duration/len(results):.1f}s (objectif: <180s)")
        print(f"{'='*70}")
        
    finally:
        enricher.close()

if __name__ == "__main__":
    main()
