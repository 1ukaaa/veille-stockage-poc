#!/usr/bin/env python3
"""
ENRICHISSEMENT v12.0 - PRODUCTION FINALE
- Filtre temporel intelligent
- Parall√©lisation sans perte de qualit√©
- Cache des extractions
"""

import sys
import json
import time
import re
import os
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
import argparse
import logging
from urllib.parse import urljoin, urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed

import httpx
import pandas as pd
from selectolax.parser import HTMLParser

from config import settings
from utils import HTTPClient, HTTPDownloadTooLarge, extract_pdf_robust, is_poor_text

logger = logging.getLogger(__name__)


def configure_logging(debug: bool):
    level = logging.DEBUG if debug else logging.INFO
    logging.basicConfig(
        level=level,
        format="%(asctime)s [%(levelname)s] %(message)s",
        datefmt="%H:%M:%S",
        force=True,
    )

class Config:
    CSE_ID = os.getenv("GOOGLE_CSE_ID")
    
    # Qualit√© maximale
    MAX_SEARCH_RESULTS = 10
    MAX_PAGES_TO_SCRAPE = 5
    MAX_PDFS_PER_PAGE = 3
    MAX_PDF_SIZE_MB = 20
    
    # Performance
    MAX_WORKERS_SCRAPING = 3  # Parall√©liser scraping
    MAX_WORKERS_GEMINI = 2    # Parall√©liser Gemini (limit√© API)
    
    # Gemini
    GEMINI_MODEL = "gemini-2.5-flash"
    MAX_OUTPUT_TOKENS = 2048
    
    # Delays
    SEARCH_DELAY = 1.0
    SCRAPE_DELAY = 0.3  # R√©duit car parall√®le
    PROJECT_DELAY = 1.5  # R√©duit de 3 √† 1.5

DEPT_NAMES = {"01": "ain", "03": "allier", "07": "ardeche", "15": "cantal", "26": "drome", "38": "isere", "42": "loire", "43": "haute-loire", "63": "puy-de-dome", "69": "rhone", "73": "savoie", "74": "haute-savoie"}

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# FILTRE TEMPOREL
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class TemporalFilter:
    """Filtre les sources par date"""
    
    @staticmethod
    def extract_year_from_url(url: str) -> Optional[int]:
        """Extrait ann√©e depuis URL"""
        # Patterns: /2024/, -2024-, _2024_
        match = re.search(r'[/_-](\d{4})[/_-]', url)
        if match:
            year = int(match.group(1))
            if 2020 <= year <= 2030:
                return year
        return None
    
    @staticmethod
    def extract_year_from_text(text: str) -> Optional[int]:
        """Extrait ann√©e depuis texte (premi√®re date trouv√©e)"""
        # Cherche dates format 2024, DD/MM/2024, etc.
        matches = re.findall(r'\b(202[0-9])\b', text[:500])
        if matches:
            return int(matches[0])
        return None
    
    @staticmethod
    def is_relevant(url: str, text: str, project_year: int) -> bool:
        """V√©rifie si source est pertinente temporellement"""
        # Fen√™tre: year-1 √† year+3
        year_min = project_year - 1
        year_max = project_year + 3
        
        # Extraire ann√©e
        year = TemporalFilter.extract_year_from_url(url)
        if not year:
            year = TemporalFilter.extract_year_from_text(text)
        
        # Si pas d'ann√©e trouv√©e: accepter (peut √™tre page sans date)
        if not year:
            return True
        
        # V√©rifier fen√™tre
        is_ok = year_min <= year <= year_max
        if not is_ok:
            logger.debug(f"      ‚úó Filtre temporel: {year} hors fen√™tre [{year_min}, {year_max}]")
        return is_ok

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# CLASSIFICATION DES SOURCES
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class SourcePriority:
    @staticmethod
    def classify_url(url: str, title: str) -> Tuple[int, str]:
        url_lower = url.lower()
        title_lower = title.lower()
        
        if "arr√™t√©" in title_lower or "arrete" in title_lower or "decision" in title_lower:
            if ".pdf" in url_lower:
                return (0, "arrete_pdf")
        
        if "consultation" in url_lower and "public" in url_lower:
            if ".gouv.fr" in url_lower:
                return (1, "consultation_html")
        
        if "mrae" in title_lower or "avis d√©lib√©r√©" in title_lower:
            if ".pdf" in url_lower:
                return (2, "avis_mrae_pdf")
        
        if "avis" in title_lower and ".pdf" in url_lower:
            return (3, "avis_pdf")
        
        if "developpement-durable.gouv.fr" in url_lower:
            return (4, "dreal_html")
        
        if ".gouv.fr" in url_lower:
            return (5, "gouv_html")
        
        return (10, "other")
    
    @staticmethod
    def get_size_limit_mb(url: str, title: str) -> float:
        priority, doc_type = SourcePriority.classify_url(url, title)
        if priority <= 3:
            return Config.MAX_PDF_SIZE_MB
        if "√©tude" in title.lower() or "etude" in title.lower() or "impact" in title.lower():
            return min(5, Config.MAX_PDF_SIZE_MB)
        return min(10, Config.MAX_PDF_SIZE_MB)
    
    @staticmethod
    def should_extract_pdf(url: str, title: str, size_mb: Optional[float]) -> bool:
        if size_mb is None:
            return True
        priority, _ = SourcePriority.classify_url(url, title)
        if priority <= 3:
            return size_mb <= Config.MAX_PDF_SIZE_MB
        limit = SourcePriority.get_size_limit_mb(url, title)
        return size_mb <= limit

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# CLIENTS
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class GoogleCustomSearch:
    def __init__(self, api_key: str, cx: str):
        if not api_key or not cx:
            raise ValueError("API_KEY et CSE_ID requis")
        self.api_key = api_key
        self.cx = cx
        self.client = httpx.Client(timeout=30)
        logger.info(f"‚úì CSE: {cx[:20]}...")
    
    def search(self, query: str, num: int = 10) -> List[Dict]:
        try:
            response = self.client.get("https://www.googleapis.com/customsearch/v1", params={"key": self.api_key, "cx": self.cx, "q": query, "num": min(num, 10)})
            response.raise_for_status()
            items = response.json().get("items", [])
            return [{"title": i.get("title", ""), "url": i.get("link", ""), "snippet": i.get("snippet", "")} for i in items[:num]]
        except Exception as e:
            logger.debug(f"Search error: {e}")
            return []
    
    def close(self):
        self.client.close()

class GeminiClient:
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.client = httpx.Client(timeout=30)
    
    def extract_from_source(self, source: Dict, commune: str, project_year: int) -> Dict:
        """Extraction depuis UNE source"""
        source_type = source.get("type", "unknown")
        content = source.get("content", "")[:8000]
        url = source.get("url", "")
        
        # Contexte selon type
        if source_type == "arrete_pdf":
            context = "ARR√äT√â PR√âFECTORAL (source la plus fiable). Contient le num√©ro PC et la date de signature."
        elif source_type == "consultation_html":
            context = "Page CONSULTATION PUBLIQUE. Contient les dates de consultation et peut mentionner le PC."
        elif "mrae" in source_type:
            context = "AVIS MRAe. L'introduction contient la date de d√©p√¥t du permis."
        else:
            context = "Document administratif officiel."
        
        # Fen√™tre temporelle
        year_min = project_year - 1
        year_max = project_year + 3
        
        prompt = f"""{context}

PROJET: {commune} (ann√©e {project_year})

FEN√äTRE TEMPORELLE: Cherche dates entre {year_min} et {year_max}

CONTENU:
{content}

Extrais:
1. Num√©ro PC (format "PC XXX YYY...")
2. Dates (format YYYY-MM-DD)

Convertir dates fran√ßaises DD/MM/YYYY ‚Üí YYYY-MM-DD
IGNORER dates hors fen√™tre {year_min}-{year_max}

JSON:
{{
  "permit_number": "string ou null",
  "deposit_date": "YYYY-MM-DD ou null",
  "issue_date": "YYYY-MM-DD ou null",
  "consultation_start": "YYYY-MM-DD ou null",
  "consultation_end": "YYYY-MM-DD ou null",
  "confidence": 0.8
}}

Confidence:
- 1.0: Arr√™t√© + PC + date signature
- 0.8: PC + 1+ date
- 0.6: Dates sans PC mais .gouv.fr
- 0.0: Rien trouv√©"""
        
        try:
            response = self.client.post(
                f"https://generativelanguage.googleapis.com/v1beta/models/{Config.GEMINI_MODEL}:generateContent",
                params={"key": self.api_key},
                json={"contents": [{"role": "user", "parts": [{"text": prompt}]}], "generationConfig": {"maxOutputTokens": Config.MAX_OUTPUT_TOKENS, "temperature": 0.0, "responseMimeType": "application/json"}}
            )
            response.raise_for_status()
            
            parts = []
            for candidate in response.json().get("candidates", []):
                for part in candidate.get("content", {}).get("parts", []):
                    if "text" in part:
                        parts.append(part["text"])
            raw_text = "".join(parts).strip()
            if not raw_text:
                return {"confidence": 0.0}
            fenced = re.match(r"```(?:json)?\s*(.*?)\s*```", raw_text, re.DOTALL)
            if fenced:
                raw_text = fenced.group(1).strip()
            
            try:
                data = json.loads(raw_text)
                data["source_url"] = url
                data["source_type"] = source_type
                return data
            except json.JSONDecodeError as exc:
                logger.debug(f"Gemini JSON parse error: {exc}")
        except Exception as e:
            logger.debug(f"Gemini error: {e}")
        
        return {"confidence": 0.0}
    
    def close(self):
        self.client.close()

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# EXTRACTEURS (avec parall√©lisation)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def scrape_html(url: str, http_client: HTTPClient) -> Optional[str]:
    try:
        html, _ = http_client.get_text(url)
        tree = HTMLParser(html)
        for tag in tree.css("script, style, nav, header, footer"):
            tag.decompose()
        if tree.body:
            return tree.body.text(separator=" ", strip=True)
    except:
        pass
    return None

def find_pdf_links(html: str, base_url: str) -> List[Dict]:
    tree = HTMLParser(html)
    pdfs = []
    for link in tree.css("a[href]"):
        href = link.attributes.get("href", "")
        if not href:
            continue
        if href.startswith("http"):
            url = href
        elif href.startswith("/"):
            parsed = urlparse(base_url)
            url = f"{parsed.scheme}://{parsed.netloc}{href}"
        else:
            url = urljoin(base_url, href)
        if ".pdf" in url.lower():
            text = link.text(strip=True)
            priority, doc_type = SourcePriority.classify_url(url, text)
            pdfs.append({"url": url, "label": text[:100], "priority": priority, "type": doc_type})
    pdfs.sort(key=lambda x: x["priority"])
    return pdfs

def extract_pdf_text(url: str, title: str, http_client: HTTPClient) -> Optional[str]:
    try:
        head_size_mb: Optional[float] = None
        try:
            head_response = http_client.head(url)
            content_length = head_response.headers.get("Content-Length")
            if content_length:
                try:
                    head_size_mb = int(content_length) / (1024 * 1024)
                except ValueError:
                    head_size_mb = None
        except Exception as head_error:
            logger.debug(f"      HEAD failed: {head_error}")
        
        if head_size_mb is not None and not SourcePriority.should_extract_pdf(url, title, head_size_mb):
            logger.debug(f"      Skip HEAD size: {head_size_mb:.1f}MB")
            return None
        
        size_limit_mb = SourcePriority.get_size_limit_mb(url, title)
        max_bytes = int(size_limit_mb * 1024 * 1024)
        
        pdf_bytes, _ = http_client.get_bytes(url, max_bytes=max_bytes)
        size_mb = len(pdf_bytes) / (1024 * 1024)
        
        if not SourcePriority.should_extract_pdf(url, title, size_mb):
            logger.debug(f"      Skip body size: {size_mb:.1f}MB")
            return None
        
        text, method = extract_pdf_robust(pdf_bytes)
        if text and not is_poor_text(text):
            return text
    except HTTPDownloadTooLarge:
        logger.debug("      Skip: t√©l√©chargement interrompu (fichier trop volumineux)")
    except Exception as e:
        logger.debug(f"      ‚úó {e}")
    return None

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# QUERIES
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def build_queries(project: Dict) -> List[str]:
    commune = project.get("commune", "")
    dept = project.get("dept", "")
    year = int(project.get("year", 2023))
    dept_name = DEPT_NAMES.get(dept, f"dept{dept}")
    
    queries = []
    
    # Ajout filtre temporel dans les queries
    year_filter = f"{year-1}..{year+3}"
    
    if commune and dept:
        queries.append(f'"{commune}" "consultation du public" "permis de construire" stockage batteries site:{dept_name}.gouv.fr daterange:{year_filter}')
    
    if commune:
        queries.append(f'"{commune}" stockage √©nergie batteries site:developpement-durable.gouv.fr')
    
    if commune:
        queries.append(f'"avis d√©lib√©r√©" "{commune}" stockage batteries filetype:pdf')
    
    if commune and dept:
        queries.append(f'"{commune}" arr√™t√© permis construire stockage {dept} filetype:pdf')
    
    return queries

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# DATACLASS
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@dataclass
class PermitData:
    permit_number: Optional[str] = None
    deposit_date: Optional[str] = None
    issue_date: Optional[str] = None
    consultation_start: Optional[str] = None
    consultation_end: Optional[str] = None
    source_url: Optional[str] = None
    source_type: Optional[str] = None
    confidence: float = 0.0
    summary: str = ""
    
    def to_csv_dict(self, project: Dict) -> Dict:
        return {**project, "permit_number": self.permit_number, "permit_deposit_date": self.deposit_date, "permit_issue_date": self.issue_date, "permit_consultation_start": self.consultation_start, "permit_consultation_end": self.consultation_end, "permit_source": self.source_url, "permit_source_type": self.source_type, "permit_confidence": self.confidence, "permit_summary": self.summary}

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# ENRICHISSEUR (avec parall√©lisation)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class PermitEnricher:
    def __init__(self, api_key: str, cse_id: str):
        self.search_client = GoogleCustomSearch(api_key, cse_id)
        self.gemini_client = GeminiClient(api_key)
        self.http_client = HTTPClient(delay=Config.SCRAPE_DELAY)
    
    def enrich(self, project: Dict) -> PermitData:
        title = project.get("project_title", "")
        commune = project.get("commune", "")
        year = int(project.get("year", 2023))
        
        logger.info(f"üîç {title[:50]}... (ann√©e {year})")
        
        # RECHERCHE
        queries = build_queries(project)
        all_sources = []
        
        for query in queries:
            results = self.search_client.search(query, Config.MAX_SEARCH_RESULTS)
            for result in results:
                url = result["url"]
                if any(s["url"] == url for s in all_sources):
                    continue
                priority, source_type = SourcePriority.classify_url(url, result["title"])
                all_sources.append({"url": url, "title": result["title"], "priority": priority, "type": source_type})
            time.sleep(Config.SEARCH_DELAY)
        
        if not all_sources:
            return PermitData(summary="Aucun r√©sultat")
        
        all_sources.sort(key=lambda x: x["priority"])
        logger.debug(f"  ‚Üí {len(all_sources)} sources")
        
        # SCRAPING PARALL√àLE
        sources_with_content = []
        
        def scrape_one(source):
            url = source["url"]
            content = None
            
            if ".pdf" in url.lower():
                content = extract_pdf_text(url, source["title"], self.http_client)
            else:
                content = scrape_html(url, self.http_client)
                if content:
                    # Filtre temporel
                    if TemporalFilter.is_relevant(url, content, year):
                        logger.debug(f"    ‚úì HTML: {len(content)} chars")
                    else:
                        content = None
            
            if content:
                return {"url": url, "type": source["type"], "content": content}
            return None
        
        with ThreadPoolExecutor(max_workers=Config.MAX_WORKERS_SCRAPING) as executor:
            futures = {executor.submit(scrape_one, s): s for s in all_sources[:Config.MAX_PAGES_TO_SCRAPE]}
            for future in as_completed(futures):
                result = future.result()
                if result:
                    sources_with_content.append(result)
        
        if not sources_with_content:
            return PermitData(summary="Scraping √©chou√©")
        
        logger.debug(f"  ‚Üí {len(sources_with_content)} sources avec contenu")
        
        # EXTRACTION GEMINI PARALL√àLE (limit√© √† 2 workers pour API)
        best_result = PermitData()
        
        def extract_one(source):
            return self.gemini_client.extract_from_source(source, commune, year)
        
        with ThreadPoolExecutor(max_workers=Config.MAX_WORKERS_GEMINI) as executor:
            futures = {executor.submit(extract_one, s): s for s in sources_with_content}
            for future in as_completed(futures):
                try:
                    result = future.result()
                    if result.get("confidence", 0) > best_result.confidence:
                        best_result = PermitData(
                            permit_number=result.get("permit_number"),
                            deposit_date=result.get("deposit_date"),
                            issue_date=result.get("issue_date"),
                            consultation_start=result.get("consultation_start"),
                            consultation_end=result.get("consultation_end"),
                            source_url=result.get("source_url"),
                            source_type=result.get("source_type"),
                            confidence=result.get("confidence", 0),
                            summary=f"Trouv√© via {result.get('source_type')}"
                        )
                        
                        # Early stop
                        if best_result.confidence >= 0.9:
                            executor.shutdown(wait=False, cancel_futures=True)
                            break
                except:
                    pass
        
        # Log
        details = []
        if best_result.permit_number:
            details.append(f"PC:{best_result.permit_number}")
        if best_result.issue_date:
            details.append(f"Obtenu:{best_result.issue_date}")
        
        status = "‚úÖ" if best_result.confidence >= 0.6 else "‚ö†Ô∏è"
        logger.info(f"  {status} {best_result.confidence:.2f} - {' | '.join(details) if details else 'N/A'}")
        
        return best_result
    
    def close(self):
        self.search_client.close()
        self.gemini_client.close()
        self.http_client.close()

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# MAIN
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def main():
    parser = argparse.ArgumentParser(description="üéØ Enrichissement v12.0 - Optimis√© + Filtr√©")
    parser.add_argument("--input", required=True)
    parser.add_argument("--output")
    parser.add_argument("--limit", type=int)
    parser.add_argument("--debug", action="store_true")
    args = parser.parse_args()
    
    configure_logging(args.debug)
    
    input_path = Path(args.input)
    if not input_path.exists() or not settings.GOOGLE_API_KEY or not Config.CSE_ID:
        logger.error("‚ùå Fichier/API manquant")
        sys.exit(1)
    
    output_path = Path(args.output) if args.output else settings.OUTPUT_DIR / "enriched" / (input_path.stem.replace("analyzed", "enriched") + ".csv")
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    df = pd.read_csv(input_path, dtype=str).fillna("")
    if args.limit:
        df = df.head(args.limit)
    
    enricher = PermitEnricher(settings.GOOGLE_API_KEY, Config.CSE_ID)
    
    try:
        start_time = time.time()
        results = []
        
        for _, row in df.iterrows():
            try:
                permit = enricher.enrich(row.to_dict())
                results.append(permit.to_csv_dict(row.to_dict()))
            except KeyboardInterrupt:
                logger.warning("‚ö†Ô∏è Interruption")
                break
            except Exception as e:
                logger.error(f"Erreur: {e}")
                results.append({**row.to_dict(), "permit_confidence": 0.0})
            
            time.sleep(Config.PROJECT_DELAY)
        
        duration = time.time() - start_time
        
        df_out = pd.DataFrame(results)
        df_out.to_csv(output_path, index=False, encoding="utf-8")
        
        found = sum(1 for r in results if r.get("permit_number"))
        high_conf = sum(1 for r in results if float(r.get("permit_confidence", 0)) >= 0.6)
        avg_conf = sum(float(r.get("permit_confidence", 0)) for r in results) / len(results) if results else 0
        
        print(f"\n{'='*70}")
        print(f"‚úÖ Permis trouv√©s: {found}/{len(results)} ({found/len(results)*100:.1f}%)")
        print(f"   Confiance ‚â• 0.6: {high_conf} ({high_conf/len(results)*100:.1f}%)")
        print(f"   Confiance moyenne: {avg_conf:.2f}")
        print(f"   Dur√©e totale: {duration/60:.1f} min")
        print(f"   Temps/projet: {duration/len(results):.1f}s (objectif: <180s)")
        print(f"{'='*70}")
        
    finally:
        enricher.close()

if __name__ == "__main__":
    main()
